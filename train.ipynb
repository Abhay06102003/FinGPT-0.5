{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n# Installs Unsloth, Xformers (Flash Attention) and all other packages\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps xformers trl peft accelerate bitsandbytes\n!pip install sec_api\n!pip install -U langchain\n!pip install -U langchain-community\n!pip install -U sentence-transformers\n!pip install -U faiss-gpu\n!pip install triton","metadata":{"execution":{"iopub.status.busy":"2024-06-28T18:58:50.463555Z","iopub.execute_input":"2024-06-28T18:58:50.463965Z","iopub.status.idle":"2024-06-28T19:00:44.497987Z","shell.execute_reply.started":"2024-06-28T18:58:50.463933Z","shell.execute_reply":"2024-06-28T19:00:44.496651Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>Summarize the following financial report. Include ALL key statistical data and metrics. Focus on:\n\n1. Revenue and profit figures.\n2. Year-over-year growth rates.\n3. Profit margins.\n4. Debt levels and ratios.\n5. Market share.\n6. Notable trends or changes.\n7. predict Future Price Gain or Loss in percentage.\n8. Summary should be concise and to the mark with relevant data.\n9. Give Overall sentiment according to data provided with Up and down trend indication.\n10. You are a good financial analyzer and analyze effectively.\n\nProvide a comprehensive yet concise summary suitable for financial professionals.\nUser will give Context and Question according to which assistant have to produce Summary and Sentiment.<|eot_id|>\n\n<|start_header_id|>user<|end_header_id|>\n### Question:\n{}\n### Context:\n{}\n\n<|eot_id|>\n### Response:\n<|start_header_id|>assistant<|end_header_id|>\n{}\n<|eot_id|>\n\"\"\"\ndef formating(example):\n    contexts = example['context']\n    questions = example['question']\n    answers = example['answer']\n    texts = []\n    for context,question,answer in zip(contexts,questions,answers):\n        text = prompt.format(question,context,answer)\n        texts.append(text)\n    return {'text':texts,}","metadata":{"execution":{"iopub.status.busy":"2024-06-28T18:56:17.396198Z","iopub.execute_input":"2024-06-28T18:56:17.396938Z","iopub.status.idle":"2024-06-28T18:56:17.404403Z","shell.execute_reply.started":"2024-06-28T18:56:17.396898Z","shell.execute_reply":"2024-06-28T18:56:17.403521Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset('virattt/financial-qa-10K',split = 'train')","metadata":{"execution":{"iopub.status.busy":"2024-06-28T18:56:17.405451Z","iopub.execute_input":"2024-06-28T18:56:17.405751Z","iopub.status.idle":"2024-06-28T18:56:21.440181Z","shell.execute_reply.started":"2024-06-28T18:56:17.405727Z","shell.execute_reply":"2024-06-28T18:56:21.439411Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/419 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c02331d1d15a457fac34d7771389c25e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.59M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b5b2a9851eb47ef936806bf97d74929"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e364cda8ac544a6ae490929ac89d671"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = dataset.map(formating,batched = True)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T18:56:21.442418Z","iopub.execute_input":"2024-06-28T18:56:21.442972Z","iopub.status.idle":"2024-06-28T18:56:21.581874Z","shell.execute_reply.started":"2024-06-28T18:56:21.442937Z","shell.execute_reply":"2024-06-28T18:56:21.580891Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06c1690a1eee4e7eab22d62838cda463"}},"metadata":{}}]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nfrom unsloth import is_bfloat16_supported\nmodel,tokenizer = FastLanguageModel.from_pretrained(\n    model_name = 'meta-llama/Meta-Llama-3-8B-Instruct',\n    max_seq_length = 1024,\n    dtype = None,\n    load_in_4bit = True,\n    token = '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'\n)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 8,\n    lora_alpha = 32,\n    lora_dropout = 0,\n    target_modules = ['q_proj','k_proj','v_proj','o_proj'],\n    bias = 'none',\n    use_gradient_checkpointing = 'unsloth',\n    random_state = 2024,\n)\n\nprint(model.print_trainable_parameters())","metadata":{"execution":{"iopub.status.busy":"2024-06-28T19:00:44.500197Z","iopub.execute_input":"2024-06-28T19:00:44.500538Z","iopub.status.idle":"2024-06-28T19:01:39.644840Z","shell.execute_reply.started":"2024-06-28T19:00:44.500504Z","shell.execute_reply":"2024-06-28T19:01:39.643863Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Ε Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2024-06-28 19:00:49.625920: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-28 19:00:49.626024: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-28 19:00:49.749248: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"370965ed023144dcb370c03ece02fc2d"}},"metadata":{}},{"name":"stdout","text":"==((====))==  Unsloth: Fast Llama patching release 2024.6\n   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.895 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.1.2. CUDA = 6.0. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. Xformers = 0.0.26.post1. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a30d7309d454062bf88e36b522b766a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/131 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bef9efe96e2b4b6f8bc96b2b51ca47a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2027971bec784a9c9887a552b72eead0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c9fa8cff2ee412fa565fff0240e2364"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"604ac28054454575b17f51bbc509a4bc"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nNot an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\nare not enabled or a bias term (like in Qwen) is used.\nUnsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 0 MLP layers.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\nimport torch\nwandb.init(project = 'Main_model',name = '1st')\n# Configure training arguments\ntrainer = SFTTrainer(\n    # The model to be fine-tuned\n    model = model,\n    # The tokenizer associated with the model\n    tokenizer = tokenizer,\n    # The dataset used for training\n    train_dataset = dataset,\n    # The field in the dataset containing the text data\n    dataset_text_field = \"text\",\n    # Maximum sequence length for the training data\n    max_seq_length = 2048,\n    # Number of processes to use for data loading\n    dataset_num_proc = 2,\n    # Whether to use sequence packing, which can speed up training for short sequences\n    packing = False,\n    args = TrainingArguments(\n        # Batch size per device during training\n        per_device_train_batch_size = 2,\n        # Number of gradient accumulation steps to perform before updating the model parameters\n        gradient_accumulation_steps = 4,\n        # Number of warmup steps for learning rate scheduler\n        warmup_steps = 5,\n        # Total number of training steps\n        max_steps = 100,\n        # Number of training epochs, can use this instead of max_steps, for this notebook its ~900 steps given the dataset\n        # num_train_epochs = 1,\n        # Learning rate for the optimizer\n        learning_rate = 2e-4,\n        # Use 16-bit floating point precision for training if bfloat16 is not supported\n        fp16 = not is_bfloat16_supported(),\n        # Use bfloat16 precision for training if supported\n        bf16 = is_bfloat16_supported(),\n        # Number of steps between logging events\n        logging_steps = 1,\n        # Optimizer to use (in this case, AdamW with 8-bit precision)\n        optim = \"adamw_8bit\",\n        # Weight decay to apply to the model parameters\n        weight_decay = 0.01,\n        # Type of learning rate scheduler to use\n        lr_scheduler_type = \"cosine\",\n        # Seed for random number generation to ensure reproducibility\n        seed = 3407,\n        # Directory to save the output models and logs\n        output_dir = \"outputs\",\n        report_to='wandb',\n        run_name = wandb.run.name,\n    ),\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T19:01:39.646141Z","iopub.execute_input":"2024-06-28T19:01:39.646776Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240628_190213-6kgss4b4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/abhay061020031/Main_model/runs/6kgss4b4' target=\"_blank\">1st</a></strong> to <a href='https://wandb.ai/abhay061020031/Main_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/abhay061020031/Main_model' target=\"_blank\">https://wandb.ai/abhay061020031/Main_model</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/abhay061020031/Main_model/runs/6kgss4b4' target=\"_blank\">https://wandb.ai/abhay061020031/Main_model/runs/6kgss4b4</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of  Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/7000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"502fa4c93a764f1ebda77aea95fe07d5"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 7,000 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 100\n \"-____-\"     Number of trainable parameters = 6,815,744\n","output_type":"stream"}]}]}